{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)-importing utile libraries and creating spark session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()\n",
    "###importing sparkContext\n",
    "from pyspark import SparkContext\n",
    "###Creating spark session \n",
    "from pyspark.sql import SparkSession        \n",
    "###initializing SparkSession\n",
    "spark=SparkSession.builder.master(\"local[3]\").appName(\"my_Kmeans_Clustering\").getOrCreate()\n",
    "import configparser\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import date, timedelta, datetime, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1)-Creating  \"properties.conf\" containing our program information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('properties.conf', 'w')\n",
    "f.write(\"[Bristol-City-bike]\")\n",
    "f.write(\"\\nInput-data=data/Bristol-city-bike.json\" )\n",
    "f.write(\"\\nOutput-data =exported/\" )\n",
    "f.write(\"\\nKmeans-level= 3\" )\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2)-Utilizing \"properties.conf\" file to retrieve paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"properties.conf\")\n",
    "input_path=config['Bristol-City-bike']['Input-data']\n",
    "output_path= config['Bristol-City-bike']['Output-data']\n",
    "kmeans_partition = config['Bristol-City-bike']['Kmeans-level']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)-Importing 'Bristol-City-bike.json' file using the output path already created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+--------------------+------+\n",
      "|             address|  latitude| longitude|                name|number|\n",
      "+--------------------+----------+----------+--------------------+------+\n",
      "|Lower River Tce /...|-27.482279|153.028723|122 - LOWER RIVER...|   122|\n",
      "|Main St / Darragh St| -27.47059|153.036046|91 - MAIN ST / DA...|    91|\n",
      "+--------------------+----------+----------+--------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bristol = spark.read.json(input_path)\n",
    "bristol.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4)-Creating a new data frame \"Kmeans-df\", consiting of \"latitude\" and \"longitude\" only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|  latitude| longitude|\n",
      "+----------+----------+\n",
      "|-27.482279|153.028723|\n",
      "| -27.47059|153.036046|\n",
      "+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Kmeans_df=bristol.select(\"latitude\",\"longitude\")\n",
    "Kmeans_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)-Applying the k-means ML method on \"Kmeans_df\" data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+----------+\n",
      "|  latitude| longitude|            features|prediction|\n",
      "+----------+----------+--------------------+----------+\n",
      "|-27.482279|153.028723|[153.028723,-27.4...|         2|\n",
      "| -27.47059|153.036046|[153.036046,-27.4...|         2|\n",
      "+----------+----------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "features = (\"longitude\",\"latitude\")\n",
    "kmeans = KMeans().setK(int(kmeans_partition)).setSeed(1)\n",
    "assembler = VectorAssembler(inputCols=features,outputCol=\"features\")\n",
    "dataset=assembler.transform(Kmeans_df)\n",
    "model = kmeans.fit(dataset)\n",
    "fitted = model.transform(dataset)\n",
    "fitted.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6)-Verifying \"fitted\" data olumn names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['latitude', 'longitude', 'features', 'prediction']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1)-determining the variables \"longitude\" and \"latitude averages per cluster, using Spark DSL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------------------+\n",
      "|prediction|      avg(latitude)|    avg(longitude)|\n",
      "+----------+-------------------+------------------+\n",
      "|         1|-27.460240636363633|153.04186302272726|\n",
      "|         2| -27.47255990624999|   153.02594553125|\n",
      "|         0|-27.481218536585374|153.00572882926832|\n",
      "+----------+-------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(approx_count_distinct(prediction)=3)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_DSL=fitted.groupBy(\"prediction\").mean(\"latitude\",\"longitude\").orderBy("prediction").show(5)\n",
    "##Making sure we have 3 clusters\n",
    "fitted.select(approx_count_distinct(\"prediction\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.2)-determining the variables \"longitude\" and \"latitude averages per cluster, using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------------------+\n",
      "|prediction|      avg(latitude)|    avg(longitude)|\n",
      "+----------+-------------------+------------------+\n",
      "|         1|-27.460240636363633|153.04186302272726|\n",
      "|         2| -27.47255990624999|   153.02594553125|\n",
      "|         0|-27.481218536585374|153.00572882926832|\n",
      "+----------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SQL
     fitted.createOrReplaceTempView("fittedSQL") # transformation du data frame en table !!!!"
    "spark.sql(""" select prediction, mean(latitude) as Mean_Latitude, 
                mean(longitude) as Mean_Longitude
                from fittedSQL
                group by prediction 
                order by prediction asc """).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: both the methods for calculating \"latitude\" and \"longitude\" averages lead to the same results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9)-exporting fitted data frame after droping \"features variable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|  latitude| longitude|prediction|\n",
      "+----------+----------+----------+\n",
      "|-27.482279|153.028723|         2|\n",
      "| -27.47059|153.036046|         2|\n",
      "+----------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fitted=fitted.drop(\"features\")\n",
    "fitted.show(2)\n",
    "fitted.write.csv('output_path/resultats.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
